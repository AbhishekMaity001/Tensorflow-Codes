{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords can be found here : https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'learning': 4, 'dog': 5, 'cats': 6, 'parrot': 7, 'hate': 8, 'cat': 9, 'am': 10, 'a': 11, 'working': 12, 'professional': 13, 'most': 14, 'often': 15, 'lean': 16, 'towards': 17, 'machine': 18, 'and': 19, 'deep': 20, 'work': 21}\n"
     ]
    }
   ],
   "source": [
    "#importing tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = ['I love my dog & Cats',\n",
    "            'I love my parrot',\n",
    "            'I hate cat',\n",
    "            'I am a working Professional',\n",
    "            'Most often i lean towards Machine Learning and Deep Learning work']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Word Indexes are  : ---->>\n",
      "\n",
      " {'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'learning': 5, 'dog': 6, 'cats': 7, 'parrot': 8, 'hate': 9, 'cat': 10, 'am': 11, 'a': 12, 'working': 13, 'professional': 14, 'most': 15, 'often': 16, 'lean': 17, 'towards': 18, 'machine': 19, 'and': 20, 'deep': 21, 'work': 22}\n",
      "\n",
      " Sequences ====> [[2, 3, 4, 6, 7], [2, 3, 4, 8], [2, 9, 10], [2, 11, 12, 13, 14], [15, 16, 2, 17, 18, 19, 5, 20, 21, 5, 22]]\n",
      "\n",
      " Padded ====>\n",
      "\n",
      " [[ 2  3  4  6  7  0  0  0  0  0  0  0]\n",
      " [ 2  3  4  8  0  0  0  0  0  0  0  0]\n",
      " [ 2  9 10  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 11 12 13 14  0  0  0  0  0  0  0]\n",
      " [15 16  2 17 18 19  5 20 21  5 22  0]]\n",
      "\n",
      " test sequence without padding :  [[2, 1, 4, 1, 22], [1, 1, 1, 1]]\n",
      "\n",
      " test sequence with padding  :  [[ 0  0  0  0  0  2  1  4  1 22]\n",
      " [ 0  0  0  0  0  0  1  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = ['I love my dog & Cats',\n",
    "            'I love my parrot',\n",
    "            'I hate cat',\n",
    "            'I am a working Professional',\n",
    "            'Most often i lean towards Machine Learning and Deep Learning work']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100,oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print('\\nThe Word Indexes are  : ---->>\\n\\n',word_index)\n",
    "\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequence,maxlen=12,padding='post',truncating='post')\n",
    "\n",
    "print('\\n Sequences ====>',sequence)\n",
    "print('\\n Padded ====>')\n",
    "print('\\n',padded)\n",
    "\n",
    "\n",
    "\n",
    "test_data = [\n",
    "    'I have my own work',\n",
    "    'today is Ria\\'s birthday'\n",
    "]\n",
    "\n",
    "test_sequence = tokenizer.texts_to_sequences(test_data)\n",
    "print('\\n test sequence without padding : ',test_sequence)\n",
    "\n",
    "test_padding = pad_sequences(test_sequence,maxlen=10)\n",
    "print('\\n test sequence with padding  : ',test_padding)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
