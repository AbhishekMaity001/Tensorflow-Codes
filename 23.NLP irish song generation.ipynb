{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaelic ( irish ) Song Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, GRU, Embedding, Bidirectional, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the irish song is separated by \\n for the new line\n",
    "data = \"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in the town of athy one jeremy lanigan ',\n",
       " ' battered away til he hadnt a pound. ',\n",
       " 'his father died and made him a man again ',\n",
       " ' left him a farm and ten acres of ground. ',\n",
       " 'he gave a grand party for friends and relations ',\n",
       " 'who didnt forget him when come to the wall, ',\n",
       " 'and if youll but listen ill make your eyes glisten ',\n",
       " 'of the rows and the ructions of lanigans ball. ',\n",
       " 'myself to be sure got free invitation, ',\n",
       " 'for all the nice girls and boys i might ask, ',\n",
       " 'and just in a minute both friends and relations ',\n",
       " 'were dancing round merry as bees round a cask. ',\n",
       " 'judy odaly, that nice little milliner, ',\n",
       " 'she tipped me a wink for to give her a call, ',\n",
       " 'and i soon arrived with peggy mcgilligan ',\n",
       " 'just in time for lanigans ball. ',\n",
       " 'there were lashings of punch and wine for the ladies, ',\n",
       " 'potatoes and cakes; there was bacon and tea, ',\n",
       " 'there were the nolans, dolans, ogradys ',\n",
       " 'courting the girls and dancing away. ',\n",
       " 'songs they went round as plenty as water, ',\n",
       " 'the harp that once sounded in taras old hall,',\n",
       " 'sweet nelly gray and the rat catchers daughter,',\n",
       " 'all singing together at lanigans ball. ',\n",
       " 'they were doing all kinds of nonsensical polkas ',\n",
       " 'all round the room in a whirligig. ',\n",
       " 'julia and i, we banished their nonsense ',\n",
       " 'and tipped them the twist of a reel and a jig. ',\n",
       " 'ach mavrone, how the girls got all mad at me ',\n",
       " 'danced til youd think the ceiling would fall. ',\n",
       " 'for i spent three weeks at brooks academy ',\n",
       " 'learning new steps for lanigans ball. ',\n",
       " 'three long weeks i spent up in dublin, ',\n",
       " 'three long weeks to learn nothing at all,',\n",
       " ' three long weeks i spent up in dublin, ',\n",
       " 'learning new steps for lanigans ball. ',\n",
       " 'she stepped out and i stepped in again, ',\n",
       " 'i stepped out and she stepped in again, ',\n",
       " 'she stepped out and i stepped in again, ',\n",
       " 'learning new steps for lanigans ball. ',\n",
       " 'boys were all merry and the girls they were hearty ',\n",
       " 'and danced all around in couples and groups, ',\n",
       " 'til an accident happened, young terrance mccarthy ',\n",
       " 'put his right leg through miss finnertys hoops. ',\n",
       " 'poor creature fainted and cried meelia murther, ',\n",
       " 'called for her brothers and gathered them all. ',\n",
       " 'carmody swore that hed go no further ',\n",
       " 'til he had satisfaction at lanigans ball. ',\n",
       " 'in the midst of the row miss kerrigan fainted, ',\n",
       " 'her cheeks at the same time as red as a rose. ',\n",
       " 'some of the lads declared she was painted, ',\n",
       " 'she took a small drop too much, i suppose. ',\n",
       " 'her sweetheart, ned morgan, so powerful and able, ',\n",
       " 'when he saw his fair colleen stretched out by the wall, ',\n",
       " 'tore the left leg from under the table ',\n",
       " 'and smashed all the chaneys at lanigans ball. ',\n",
       " 'boys, oh boys, twas then there were runctions. ',\n",
       " 'myself got a lick from big phelim mchugh. ',\n",
       " 'i soon replied to his introduction ',\n",
       " 'and kicked up a terrible hullabaloo. ',\n",
       " 'old casey, the piper, was near being strangled. ',\n",
       " 'they squeezed up his pipes, bellows, chanters and all. ',\n",
       " 'the girls, in their ribbons, they got all entangled ',\n",
       " 'and that put an end to lanigans ball.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.lower().split('\\n')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 26, 61, 60, 262, 13, 9, 10]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
      "263\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "seq  = tokenizer.texts_to_sequences(data)\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the town of athy one jeremy lanigan '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 66, 8, 67, 68, 69, 70]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([data[0]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in data :\n",
    "    seq = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(seq)):\n",
    "        n_gram_sequence = seq[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2],\n",
       " [4, 2, 66],\n",
       " [4, 2, 66, 8],\n",
       " [4, 2, 66, 8, 67],\n",
       " [4, 2, 66, 8, 67, 68],\n",
       " [4, 2, 66, 8, 67, 68, 69],\n",
       " [4, 2, 66, 8, 67, 68, 69, 70],\n",
       " [71, 40],\n",
       " [71, 40, 20],\n",
       " [71, 40, 20, 21],\n",
       " [71, 40, 20, 21, 72],\n",
       " [71, 40, 20, 21, 72, 3],\n",
       " [71, 40, 20, 21, 72, 3, 73],\n",
       " [16, 74],\n",
       " [16, 74, 75],\n",
       " [16, 74, 75, 1],\n",
       " [16, 74, 75, 1, 76],\n",
       " [16, 74, 75, 1, 76, 33],\n",
       " [16, 74, 75, 1, 76, 33, 3],\n",
       " [16, 74, 75, 1, 76, 33, 3, 77],\n",
       " [16, 74, 75, 1, 76, 33, 3, 77, 22],\n",
       " [41, 33],\n",
       " [41, 33, 3],\n",
       " [41, 33, 3, 78],\n",
       " [41, 33, 3, 78, 1],\n",
       " [41, 33, 3, 78, 1, 79],\n",
       " [41, 33, 3, 78, 1, 79, 80],\n",
       " [41, 33, 3, 78, 1, 79, 80, 8],\n",
       " [41, 33, 3, 78, 1, 79, 80, 8, 81],\n",
       " [21, 82],\n",
       " [21, 82, 3],\n",
       " [21, 82, 3, 83],\n",
       " [21, 82, 3, 83, 84],\n",
       " [21, 82, 3, 83, 84, 7],\n",
       " [21, 82, 3, 83, 84, 7, 42],\n",
       " [21, 82, 3, 83, 84, 7, 42, 1],\n",
       " [21, 82, 3, 83, 84, 7, 42, 1, 43],\n",
       " [85, 86],\n",
       " [85, 86, 87],\n",
       " [85, 86, 87, 33],\n",
       " [85, 86, 87, 33, 44],\n",
       " [85, 86, 87, 33, 44, 88],\n",
       " [85, 86, 87, 33, 44, 88, 13],\n",
       " [85, 86, 87, 33, 44, 88, 13, 2],\n",
       " [85, 86, 87, 33, 44, 88, 13, 2, 45],\n",
       " [1, 89],\n",
       " [1, 89, 90],\n",
       " [1, 89, 90, 91],\n",
       " [1, 89, 90, 91, 92],\n",
       " [1, 89, 90, 91, 92, 93],\n",
       " [1, 89, 90, 91, 92, 93, 94],\n",
       " [1, 89, 90, 91, 92, 93, 94, 95],\n",
       " [1, 89, 90, 91, 92, 93, 94, 95, 96],\n",
       " [1, 89, 90, 91, 92, 93, 94, 95, 96, 97],\n",
       " [8, 2],\n",
       " [8, 2, 98],\n",
       " [8, 2, 98, 1],\n",
       " [8, 2, 98, 1, 2],\n",
       " [8, 2, 98, 1, 2, 99],\n",
       " [8, 2, 98, 1, 2, 99, 8],\n",
       " [8, 2, 98, 1, 2, 99, 8, 9],\n",
       " [8, 2, 98, 1, 2, 99, 8, 9, 10],\n",
       " [46, 13],\n",
       " [46, 13, 100],\n",
       " [46, 13, 100, 101],\n",
       " [46, 13, 100, 101, 23],\n",
       " [46, 13, 100, 101, 23, 102],\n",
       " [46, 13, 100, 101, 23, 102, 103],\n",
       " [7, 5],\n",
       " [7, 5, 2],\n",
       " [7, 5, 2, 47],\n",
       " [7, 5, 2, 47, 17],\n",
       " [7, 5, 2, 47, 17, 1],\n",
       " [7, 5, 2, 47, 17, 1, 24],\n",
       " [7, 5, 2, 47, 17, 1, 24, 6],\n",
       " [7, 5, 2, 47, 17, 1, 24, 6, 104],\n",
       " [7, 5, 2, 47, 17, 1, 24, 6, 104, 105],\n",
       " [1, 48],\n",
       " [1, 48, 4],\n",
       " [1, 48, 4, 3],\n",
       " [1, 48, 4, 3, 106],\n",
       " [1, 48, 4, 3, 106, 107],\n",
       " [1, 48, 4, 3, 106, 107, 42],\n",
       " [1, 48, 4, 3, 106, 107, 42, 1],\n",
       " [1, 48, 4, 3, 106, 107, 42, 1, 43],\n",
       " [11, 49],\n",
       " [11, 49, 25],\n",
       " [11, 49, 25, 50],\n",
       " [11, 49, 25, 50, 18],\n",
       " [11, 49, 25, 50, 18, 108],\n",
       " [11, 49, 25, 50, 18, 108, 25],\n",
       " [11, 49, 25, 50, 18, 108, 25, 3],\n",
       " [11, 49, 25, 50, 18, 108, 25, 3, 109],\n",
       " [110, 111],\n",
       " [110, 111, 26],\n",
       " [110, 111, 26, 47],\n",
       " [110, 111, 26, 47, 112],\n",
       " [110, 111, 26, 47, 112, 113],\n",
       " [14, 51],\n",
       " [14, 51, 52],\n",
       " [14, 51, 52, 3],\n",
       " [14, 51, 52, 3, 114],\n",
       " [14, 51, 52, 3, 114, 7],\n",
       " [14, 51, 52, 3, 114, 7, 13],\n",
       " [14, 51, 52, 3, 114, 7, 13, 115],\n",
       " [14, 51, 52, 3, 114, 7, 13, 115, 27],\n",
       " [14, 51, 52, 3, 114, 7, 13, 115, 27, 3],\n",
       " [14, 51, 52, 3, 114, 7, 13, 115, 27, 3, 116],\n",
       " [1, 6],\n",
       " [1, 6, 53],\n",
       " [1, 6, 53, 117],\n",
       " [1, 6, 53, 117, 118],\n",
       " [1, 6, 53, 117, 118, 119],\n",
       " [1, 6, 53, 117, 118, 119, 120],\n",
       " [48, 4],\n",
       " [48, 4, 54],\n",
       " [48, 4, 54, 7],\n",
       " [48, 4, 54, 7, 9],\n",
       " [48, 4, 54, 7, 9, 10],\n",
       " [28, 11],\n",
       " [28, 11, 121],\n",
       " [28, 11, 121, 8],\n",
       " [28, 11, 121, 8, 122],\n",
       " [28, 11, 121, 8, 122, 1],\n",
       " [28, 11, 121, 8, 122, 1, 123],\n",
       " [28, 11, 121, 8, 122, 1, 123, 7],\n",
       " [28, 11, 121, 8, 122, 1, 123, 7, 2],\n",
       " [28, 11, 121, 8, 122, 1, 123, 7, 2, 124],\n",
       " [125, 1],\n",
       " [125, 1, 126],\n",
       " [125, 1, 126, 28],\n",
       " [125, 1, 126, 28, 34],\n",
       " [125, 1, 126, 28, 34, 127],\n",
       " [125, 1, 126, 28, 34, 127, 1],\n",
       " [125, 1, 126, 28, 34, 127, 1, 128],\n",
       " [28, 11],\n",
       " [28, 11, 2],\n",
       " [28, 11, 2, 129],\n",
       " [28, 11, 2, 129, 130],\n",
       " [28, 11, 2, 129, 130, 131],\n",
       " [132, 2],\n",
       " [132, 2, 17],\n",
       " [132, 2, 17, 1],\n",
       " [132, 2, 17, 1, 49],\n",
       " [132, 2, 17, 1, 49, 40],\n",
       " [133, 19],\n",
       " [133, 19, 134],\n",
       " [133, 19, 134, 25],\n",
       " [133, 19, 134, 25, 18],\n",
       " [133, 19, 134, 25, 18, 135],\n",
       " [133, 19, 134, 25, 18, 135, 18],\n",
       " [133, 19, 134, 25, 18, 135, 18, 136],\n",
       " [2, 137],\n",
       " [2, 137, 26],\n",
       " [2, 137, 26, 138],\n",
       " [2, 137, 26, 138, 139],\n",
       " [2, 137, 26, 138, 139, 4],\n",
       " [2, 137, 26, 138, 139, 4, 140],\n",
       " [2, 137, 26, 138, 139, 4, 140, 55],\n",
       " [2, 137, 26, 138, 139, 4, 140, 55, 141],\n",
       " [142, 143],\n",
       " [142, 143, 144],\n",
       " [142, 143, 144, 1],\n",
       " [142, 143, 144, 1, 2],\n",
       " [142, 143, 144, 1, 2, 145],\n",
       " [142, 143, 144, 1, 2, 145, 146],\n",
       " [142, 143, 144, 1, 2, 145, 146, 147],\n",
       " [5, 148],\n",
       " [5, 148, 149],\n",
       " [5, 148, 149, 12],\n",
       " [5, 148, 149, 12, 9],\n",
       " [5, 148, 149, 12, 9, 10],\n",
       " [19, 11],\n",
       " [19, 11, 150],\n",
       " [19, 11, 150, 5],\n",
       " [19, 11, 150, 5, 151],\n",
       " [19, 11, 150, 5, 151, 8],\n",
       " [19, 11, 150, 5, 151, 8, 152],\n",
       " [19, 11, 150, 5, 151, 8, 152, 153],\n",
       " [5, 25],\n",
       " [5, 25, 2],\n",
       " [5, 25, 2, 154],\n",
       " [5, 25, 2, 154, 4],\n",
       " [5, 25, 2, 154, 4, 3],\n",
       " [5, 25, 2, 154, 4, 3, 155],\n",
       " [156, 1],\n",
       " [156, 1, 6],\n",
       " [156, 1, 6, 157],\n",
       " [156, 1, 6, 157, 158],\n",
       " [156, 1, 6, 157, 158, 56],\n",
       " [156, 1, 6, 157, 158, 56, 159],\n",
       " [1, 51],\n",
       " [1, 51, 57],\n",
       " [1, 51, 57, 2],\n",
       " [1, 51, 57, 2, 160],\n",
       " [1, 51, 57, 2, 160, 8],\n",
       " [1, 51, 57, 2, 160, 8, 3],\n",
       " [1, 51, 57, 2, 160, 8, 3, 161],\n",
       " [1, 51, 57, 2, 160, 8, 3, 161, 1],\n",
       " [1, 51, 57, 2, 160, 8, 3, 161, 1, 3],\n",
       " [1, 51, 57, 2, 160, 8, 3, 161, 1, 3, 162],\n",
       " [163, 164],\n",
       " [163, 164, 165],\n",
       " [163, 164, 165, 2],\n",
       " [163, 164, 165, 2, 17],\n",
       " [163, 164, 165, 2, 17, 23],\n",
       " [163, 164, 165, 2, 17, 23, 5],\n",
       " [163, 164, 165, 2, 17, 23, 5, 166],\n",
       " [163, 164, 165, 2, 17, 23, 5, 166, 12],\n",
       " [163, 164, 165, 2, 17, 23, 5, 166, 12, 52],\n",
       " [58, 20],\n",
       " [58, 20, 167],\n",
       " [58, 20, 167, 168],\n",
       " [58, 20, 167, 168, 2],\n",
       " [58, 20, 167, 168, 2, 169],\n",
       " [58, 20, 167, 168, 2, 169, 170],\n",
       " [58, 20, 167, 168, 2, 169, 170, 171],\n",
       " [7, 6],\n",
       " [7, 6, 35],\n",
       " [7, 6, 35, 29],\n",
       " [7, 6, 35, 29, 30],\n",
       " [7, 6, 35, 29, 30, 12],\n",
       " [7, 6, 35, 29, 30, 12, 172],\n",
       " [7, 6, 35, 29, 30, 12, 172, 173],\n",
       " [36, 37],\n",
       " [36, 37, 38],\n",
       " [36, 37, 38, 7],\n",
       " [36, 37, 38, 7, 9],\n",
       " [36, 37, 38, 7, 9, 10],\n",
       " [29, 39],\n",
       " [29, 39, 30],\n",
       " [29, 39, 30, 6],\n",
       " [29, 39, 30, 6, 35],\n",
       " [29, 39, 30, 6, 35, 31],\n",
       " [29, 39, 30, 6, 35, 31, 4],\n",
       " [29, 39, 30, 6, 35, 31, 4, 59],\n",
       " [29, 39],\n",
       " [29, 39, 30],\n",
       " [29, 39, 30, 13],\n",
       " [29, 39, 30, 13, 174],\n",
       " [29, 39, 30, 13, 174, 175],\n",
       " [29, 39, 30, 13, 174, 175, 12],\n",
       " [29, 39, 30, 13, 174, 175, 12, 5],\n",
       " [29, 39],\n",
       " [29, 39, 30],\n",
       " [29, 39, 30, 6],\n",
       " [29, 39, 30, 6, 35],\n",
       " [29, 39, 30, 6, 35, 31],\n",
       " [29, 39, 30, 6, 35, 31, 4],\n",
       " [29, 39, 30, 6, 35, 31, 4, 59],\n",
       " [36, 37],\n",
       " [36, 37, 38],\n",
       " [36, 37, 38, 7],\n",
       " [36, 37, 38, 7, 9],\n",
       " [36, 37, 38, 7, 9, 10],\n",
       " [14, 15],\n",
       " [14, 15, 32],\n",
       " [14, 15, 32, 1],\n",
       " [14, 15, 32, 1, 6],\n",
       " [14, 15, 32, 1, 6, 15],\n",
       " [14, 15, 32, 1, 6, 15, 4],\n",
       " [14, 15, 32, 1, 6, 15, 4, 22],\n",
       " [6, 15],\n",
       " [6, 15, 32],\n",
       " [6, 15, 32, 1],\n",
       " [6, 15, 32, 1, 14],\n",
       " [6, 15, 32, 1, 14, 15],\n",
       " [6, 15, 32, 1, 14, 15, 4],\n",
       " [6, 15, 32, 1, 14, 15, 4, 22],\n",
       " [14, 15],\n",
       " [14, 15, 32],\n",
       " [14, 15, 32, 1],\n",
       " [14, 15, 32, 1, 6],\n",
       " [14, 15, 32, 1, 6, 15],\n",
       " [14, 15, 32, 1, 6, 15, 4],\n",
       " [14, 15, 32, 1, 6, 15, 4, 22],\n",
       " [36, 37],\n",
       " [36, 37, 38],\n",
       " [36, 37, 38, 7],\n",
       " [36, 37, 38, 7, 9],\n",
       " [36, 37, 38, 7, 9, 10],\n",
       " [24, 11],\n",
       " [24, 11, 5],\n",
       " [24, 11, 5, 50],\n",
       " [24, 11, 5, 50, 1],\n",
       " [24, 11, 5, 50, 1, 2],\n",
       " [24, 11, 5, 50, 1, 2, 17],\n",
       " [24, 11, 5, 50, 1, 2, 17, 19],\n",
       " [24, 11, 5, 50, 1, 2, 17, 19, 11],\n",
       " [24, 11, 5, 50, 1, 2, 17, 19, 11, 176],\n",
       " [1, 58],\n",
       " [1, 58, 5],\n",
       " [1, 58, 5, 177],\n",
       " [1, 58, 5, 177, 4],\n",
       " [1, 58, 5, 177, 4, 178],\n",
       " [1, 58, 5, 177, 4, 178, 1],\n",
       " [1, 58, 5, 177, 4, 178, 1, 179],\n",
       " [20, 60],\n",
       " [20, 60, 180],\n",
       " [20, 60, 180, 181],\n",
       " [20, 60, 180, 181, 182],\n",
       " [20, 60, 180, 181, 182, 183],\n",
       " [20, 60, 180, 181, 182, 183, 184],\n",
       " [61, 16],\n",
       " [61, 16, 185],\n",
       " [61, 16, 185, 62],\n",
       " [61, 16, 185, 62, 186],\n",
       " [61, 16, 185, 62, 186, 63],\n",
       " [61, 16, 185, 62, 186, 63, 187],\n",
       " [61, 16, 185, 62, 186, 63, 187, 188],\n",
       " [189, 190],\n",
       " [189, 190, 64],\n",
       " [189, 190, 64, 1],\n",
       " [189, 190, 64, 1, 191],\n",
       " [189, 190, 64, 1, 191, 192],\n",
       " [189, 190, 64, 1, 191, 192, 193],\n",
       " [194, 7],\n",
       " [194, 7, 27],\n",
       " [194, 7, 27, 195],\n",
       " [194, 7, 27, 195, 1],\n",
       " [194, 7, 27, 195, 1, 196],\n",
       " [194, 7, 27, 195, 1, 196, 57],\n",
       " [194, 7, 27, 195, 1, 196, 57, 5],\n",
       " [197, 198],\n",
       " [197, 198, 26],\n",
       " [197, 198, 26, 199],\n",
       " [197, 198, 26, 199, 200],\n",
       " [197, 198, 26, 199, 200, 201],\n",
       " [197, 198, 26, 199, 200, 201, 202],\n",
       " [20, 21],\n",
       " [20, 21, 203],\n",
       " [20, 21, 203, 204],\n",
       " [20, 21, 203, 204, 12],\n",
       " [20, 21, 203, 204, 12, 9],\n",
       " [20, 21, 203, 204, 12, 9, 10],\n",
       " [4, 2],\n",
       " [4, 2, 205],\n",
       " [4, 2, 205, 8],\n",
       " [4, 2, 205, 8, 2],\n",
       " [4, 2, 205, 8, 2, 206],\n",
       " [4, 2, 205, 8, 2, 206, 63],\n",
       " [4, 2, 205, 8, 2, 206, 63, 207],\n",
       " [4, 2, 205, 8, 2, 206, 63, 207, 64],\n",
       " [27, 208],\n",
       " [27, 208, 12],\n",
       " [27, 208, 12, 2],\n",
       " [27, 208, 12, 2, 209],\n",
       " [27, 208, 12, 2, 209, 54],\n",
       " [27, 208, 12, 2, 209, 54, 18],\n",
       " [27, 208, 12, 2, 209, 54, 18, 210],\n",
       " [27, 208, 12, 2, 209, 54, 18, 210, 18],\n",
       " [27, 208, 12, 2, 209, 54, 18, 210, 18, 3],\n",
       " [27, 208, 12, 2, 209, 54, 18, 210, 18, 3, 211],\n",
       " [212, 8],\n",
       " [212, 8, 2],\n",
       " [212, 8, 2, 213],\n",
       " [212, 8, 2, 213, 214],\n",
       " [212, 8, 2, 213, 214, 14],\n",
       " [212, 8, 2, 213, 214, 14, 34],\n",
       " [212, 8, 2, 213, 214, 14, 34, 215],\n",
       " [14, 216],\n",
       " [14, 216, 3],\n",
       " [14, 216, 3, 217],\n",
       " [14, 216, 3, 217, 218],\n",
       " [14, 216, 3, 217, 218, 219],\n",
       " [14, 216, 3, 217, 218, 219, 220],\n",
       " [14, 216, 3, 217, 218, 219, 220, 6],\n",
       " [14, 216, 3, 217, 218, 219, 220, 6, 221],\n",
       " [27, 222],\n",
       " [27, 222, 223],\n",
       " [27, 222, 223, 224],\n",
       " [27, 222, 223, 224, 225],\n",
       " [27, 222, 223, 224, 225, 226],\n",
       " [27, 222, 223, 224, 225, 226, 1],\n",
       " [27, 222, 223, 224, 225, 226, 1, 227],\n",
       " [44, 21],\n",
       " [44, 21, 228],\n",
       " [44, 21, 228, 16],\n",
       " [44, 21, 228, 16, 229],\n",
       " [44, 21, 228, 16, 229, 230],\n",
       " [44, 21, 228, 16, 229, 230, 231],\n",
       " [44, 21, 228, 16, 229, 230, 231, 32],\n",
       " [44, 21, 228, 16, 229, 230, 231, 32, 232],\n",
       " [44, 21, 228, 16, 229, 230, 231, 32, 232, 2],\n",
       " [44, 21, 228, 16, 229, 230, 231, 32, 232, 2, 45],\n",
       " [233, 2],\n",
       " [233, 2, 41],\n",
       " [233, 2, 41, 62],\n",
       " [233, 2, 41, 62, 65],\n",
       " [233, 2, 41, 62, 65, 234],\n",
       " [233, 2, 41, 62, 65, 234, 2],\n",
       " [233, 2, 41, 62, 65, 234, 2, 235],\n",
       " [1, 236],\n",
       " [1, 236, 5],\n",
       " [1, 236, 5, 2],\n",
       " [1, 236, 5, 2, 237],\n",
       " [1, 236, 5, 2, 237, 12],\n",
       " [1, 236, 5, 2, 237, 12, 9],\n",
       " [1, 236, 5, 2, 237, 12, 9, 10],\n",
       " [24, 238],\n",
       " [24, 238, 24],\n",
       " [24, 238, 24, 239],\n",
       " [24, 238, 24, 239, 240],\n",
       " [24, 238, 24, 239, 240, 28],\n",
       " [24, 238, 24, 239, 240, 28, 11],\n",
       " [24, 238, 24, 239, 240, 28, 11, 241],\n",
       " [46, 23],\n",
       " [46, 23, 3],\n",
       " [46, 23, 3, 242],\n",
       " [46, 23, 3, 242, 65],\n",
       " [46, 23, 3, 242, 65, 243],\n",
       " [46, 23, 3, 242, 65, 243, 244],\n",
       " [46, 23, 3, 242, 65, 243, 244, 245],\n",
       " [6, 53],\n",
       " [6, 53, 246],\n",
       " [6, 53, 246, 13],\n",
       " [6, 53, 246, 13, 16],\n",
       " [6, 53, 246, 13, 16, 247],\n",
       " [1, 248],\n",
       " [1, 248, 31],\n",
       " [1, 248, 31, 3],\n",
       " [1, 248, 31, 3, 249],\n",
       " [1, 248, 31, 3, 249, 250],\n",
       " [55, 251],\n",
       " [55, 251, 2],\n",
       " [55, 251, 2, 252],\n",
       " [55, 251, 2, 252, 34],\n",
       " [55, 251, 2, 252, 34, 253],\n",
       " [55, 251, 2, 252, 34, 253, 254],\n",
       " [55, 251, 2, 252, 34, 253, 254, 255],\n",
       " [19, 256],\n",
       " [19, 256, 31],\n",
       " [19, 256, 31, 16],\n",
       " [19, 256, 31, 16, 257],\n",
       " [19, 256, 31, 16, 257, 258],\n",
       " [19, 256, 31, 16, 257, 258, 259],\n",
       " [19, 256, 31, 16, 257, 258, 259, 1],\n",
       " [19, 256, 31, 16, 257, 258, 259, 1, 5],\n",
       " [2, 17],\n",
       " [2, 17, 4],\n",
       " [2, 17, 4, 56],\n",
       " [2, 17, 4, 56, 260],\n",
       " [2, 17, 4, 56, 260, 19],\n",
       " [2, 17, 4, 56, 260, 19, 23],\n",
       " [2, 17, 4, 56, 260, 19, 23, 5],\n",
       " [2, 17, 4, 56, 260, 19, 23, 5, 261],\n",
       " [1, 26],\n",
       " [1, 26, 61],\n",
       " [1, 26, 61, 60],\n",
       " [1, 26, 61, 60, 262],\n",
       " [1, 26, 61, 60, 262, 13],\n",
       " [1, 26, 61, 60, 262, 13, 9],\n",
       " [1, 26, 61, 60, 262, 13, 9, 10]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(i) for i in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   4,   2],\n",
       "       [  0,   0,   0, ...,   4,   2,  66],\n",
       "       [  0,   0,   0, ...,   2,  66,   8],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  60, 262,  13],\n",
       "       [  0,   0,   0, ..., 262,  13,   9],\n",
       "       [  0,   0,   0, ...,  13,   9,  10]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences = np.array(pad_sequences(input_sequences,maxlen=11,padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   4],\n",
       "       [  0,   0,   0, ...,   0,   4,   2],\n",
       "       [  0,   0,   0, ...,   4,   2,  66],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  61,  60, 262],\n",
       "       [  0,   0,   0, ...,  60, 262,  13],\n",
       "       [  0,   0,   0, ..., 262,  13,   9]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs , labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = tf.keras.utils.to_categorical(labels, num_classes=len(tokenizer.word_index)+1) # doing one hot encoding\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(453, 263)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  4,  2, 66,  8, 67])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(453, 10)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 453 samples\n",
      "Epoch 1/500\n",
      "453/453 [==============================] - 10s 22ms/sample - loss: 5.5688 - accuracy: 0.0243\n",
      "Epoch 2/500\n",
      "453/453 [==============================] - 0s 193us/sample - loss: 5.5435 - accuracy: 0.0530\n",
      "Epoch 3/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 5.4775 - accuracy: 0.0530\n",
      "Epoch 4/500\n",
      "453/453 [==============================] - 0s 197us/sample - loss: 5.3004 - accuracy: 0.0508\n",
      "Epoch 5/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 5.1401 - accuracy: 0.0508\n",
      "Epoch 6/500\n",
      "453/453 [==============================] - 0s 204us/sample - loss: 5.0757 - accuracy: 0.0508\n",
      "Epoch 7/500\n",
      "453/453 [==============================] - 0s 204us/sample - loss: 5.0426 - accuracy: 0.0464\n",
      "Epoch 8/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 5.0164 - accuracy: 0.0486\n",
      "Epoch 9/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 4.9890 - accuracy: 0.0419\n",
      "Epoch 10/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 4.9511 - accuracy: 0.0530\n",
      "Epoch 11/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 4.9010 - accuracy: 0.0574\n",
      "Epoch 12/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 4.8532 - accuracy: 0.0596\n",
      "Epoch 13/500\n",
      "453/453 [==============================] - 0s 195us/sample - loss: 4.7953 - accuracy: 0.0618\n",
      "Epoch 14/500\n",
      "453/453 [==============================] - 0s 204us/sample - loss: 4.7469 - accuracy: 0.0574\n",
      "Epoch 15/500\n",
      "453/453 [==============================] - 0s 186us/sample - loss: 4.7009 - accuracy: 0.0640\n",
      "Epoch 16/500\n",
      "453/453 [==============================] - 0s 197us/sample - loss: 4.6544 - accuracy: 0.0640\n",
      "Epoch 17/500\n",
      "453/453 [==============================] - 0s 200us/sample - loss: 4.6152 - accuracy: 0.0728\n",
      "Epoch 18/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 4.5799 - accuracy: 0.0728\n",
      "Epoch 19/500\n",
      "453/453 [==============================] - ETA: 0s - loss: 4.4361 - accuracy: 0.10 - 0s 199us/sample - loss: 4.5428 - accuracy: 0.0795\n",
      "Epoch 20/500\n",
      "453/453 [==============================] - 0s 198us/sample - loss: 4.5088 - accuracy: 0.0817\n",
      "Epoch 21/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 4.4799 - accuracy: 0.0728\n",
      "Epoch 22/500\n",
      "453/453 [==============================] - 0s 197us/sample - loss: 4.4510 - accuracy: 0.0883\n",
      "Epoch 23/500\n",
      "453/453 [==============================] - 0s 200us/sample - loss: 4.4176 - accuracy: 0.0927\n",
      "Epoch 24/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 4.3733 - accuracy: 0.0883\n",
      "Epoch 25/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 4.3349 - accuracy: 0.0839\n",
      "Epoch 26/500\n",
      "453/453 [==============================] - 0s 197us/sample - loss: 4.3007 - accuracy: 0.0861\n",
      "Epoch 27/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 4.2739 - accuracy: 0.0927\n",
      "Epoch 28/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 4.2409 - accuracy: 0.0949\n",
      "Epoch 29/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 4.2045 - accuracy: 0.1082\n",
      "Epoch 30/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 4.1765 - accuracy: 0.0949\n",
      "Epoch 31/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 4.1434 - accuracy: 0.1015\n",
      "Epoch 32/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 4.1055 - accuracy: 0.1104\n",
      "Epoch 33/500\n",
      "453/453 [==============================] - 0s 217us/sample - loss: 4.0716 - accuracy: 0.1214\n",
      "Epoch 34/500\n",
      "453/453 [==============================] - 0s 227us/sample - loss: 4.0400 - accuracy: 0.1325\n",
      "Epoch 35/500\n",
      "453/453 [==============================] - 0s 227us/sample - loss: 4.0054 - accuracy: 0.1391\n",
      "Epoch 36/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 3.9713 - accuracy: 0.1280\n",
      "Epoch 37/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 3.9283 - accuracy: 0.1479\n",
      "Epoch 38/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 3.9162 - accuracy: 0.1523\n",
      "Epoch 39/500\n",
      "453/453 [==============================] - 0s 200us/sample - loss: 3.8725 - accuracy: 0.1589\n",
      "Epoch 40/500\n",
      "453/453 [==============================] - 0s 198us/sample - loss: 3.8407 - accuracy: 0.1567\n",
      "Epoch 41/500\n",
      "453/453 [==============================] - 0s 209us/sample - loss: 3.8030 - accuracy: 0.1700\n",
      "Epoch 42/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 3.7641 - accuracy: 0.1656\n",
      "Epoch 43/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 3.7298 - accuracy: 0.1678\n",
      "Epoch 44/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 3.7037 - accuracy: 0.1766\n",
      "Epoch 45/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 3.6701 - accuracy: 0.1943\n",
      "Epoch 46/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 3.6398 - accuracy: 0.1987\n",
      "Epoch 47/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 3.6090 - accuracy: 0.1965\n",
      "Epoch 48/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 3.5727 - accuracy: 0.2252\n",
      "Epoch 49/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 3.5334 - accuracy: 0.2406\n",
      "Epoch 50/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 3.4968 - accuracy: 0.2362\n",
      "Epoch 51/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 3.4641 - accuracy: 0.2406\n",
      "Epoch 52/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 3.4415 - accuracy: 0.2428\n",
      "Epoch 53/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 3.4137 - accuracy: 0.2561\n",
      "Epoch 54/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 3.3748 - accuracy: 0.2627\n",
      "Epoch 55/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 3.3465 - accuracy: 0.2826\n",
      "Epoch 56/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 3.3163 - accuracy: 0.2848\n",
      "Epoch 57/500\n",
      "453/453 [==============================] - 0s 200us/sample - loss: 3.2775 - accuracy: 0.3002\n",
      "Epoch 58/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 3.2464 - accuracy: 0.3179\n",
      "Epoch 59/500\n",
      "453/453 [==============================] - 0s 202us/sample - loss: 3.2128 - accuracy: 0.3201\n",
      "Epoch 60/500\n",
      "453/453 [==============================] - 0s 195us/sample - loss: 3.2049 - accuracy: 0.3201\n",
      "Epoch 61/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 3.1976 - accuracy: 0.3267\n",
      "Epoch 62/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 3.1672 - accuracy: 0.3311\n",
      "Epoch 63/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 3.1307 - accuracy: 0.3422\n",
      "Epoch 64/500\n",
      "453/453 [==============================] - 0s 202us/sample - loss: 3.0899 - accuracy: 0.3466\n",
      "Epoch 65/500\n",
      "453/453 [==============================] - 0s 211us/sample - loss: 3.0428 - accuracy: 0.3731\n",
      "Epoch 66/500\n",
      "453/453 [==============================] - 0s 214us/sample - loss: 3.0061 - accuracy: 0.3576\n",
      "Epoch 67/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 2.9720 - accuracy: 0.3819\n",
      "Epoch 68/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 2.9410 - accuracy: 0.3951\n",
      "Epoch 69/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 2.9113 - accuracy: 0.4062\n",
      "Epoch 70/500\n",
      "453/453 [==============================] - 0s 207us/sample - loss: 2.8880 - accuracy: 0.4106\n",
      "Epoch 71/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 2.8698 - accuracy: 0.4216\n",
      "Epoch 72/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 2.8449 - accuracy: 0.4172\n",
      "Epoch 73/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 2.8097 - accuracy: 0.4283\n",
      "Epoch 74/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 2.7963 - accuracy: 0.4327\n",
      "Epoch 75/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 2.7565 - accuracy: 0.4547\n",
      "Epoch 76/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 2.7274 - accuracy: 0.4680\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 194us/sample - loss: 2.7030 - accuracy: 0.4680\n",
      "Epoch 78/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 2.6799 - accuracy: 0.4746\n",
      "Epoch 79/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 2.6511 - accuracy: 0.4967\n",
      "Epoch 80/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 2.6254 - accuracy: 0.5121\n",
      "Epoch 81/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 2.6045 - accuracy: 0.5033\n",
      "Epoch 82/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 2.5681 - accuracy: 0.5166\n",
      "Epoch 83/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 2.5445 - accuracy: 0.5254\n",
      "Epoch 84/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 2.5313 - accuracy: 0.5276\n",
      "Epoch 85/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 2.5072 - accuracy: 0.5519\n",
      "Epoch 86/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 2.4814 - accuracy: 0.5276\n",
      "Epoch 87/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 2.4509 - accuracy: 0.5430\n",
      "Epoch 88/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 2.4217 - accuracy: 0.5629\n",
      "Epoch 89/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 2.4011 - accuracy: 0.5762\n",
      "Epoch 90/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 2.3766 - accuracy: 0.5784\n",
      "Epoch 91/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 2.3537 - accuracy: 0.5740\n",
      "Epoch 92/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 2.3271 - accuracy: 0.5938\n",
      "Epoch 93/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 2.3109 - accuracy: 0.6071\n",
      "Epoch 94/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 2.2928 - accuracy: 0.5938\n",
      "Epoch 95/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 2.2733 - accuracy: 0.5982\n",
      "Epoch 96/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 2.2528 - accuracy: 0.6004\n",
      "Epoch 97/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 2.2553 - accuracy: 0.5894\n",
      "Epoch 98/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 2.2500 - accuracy: 0.5806\n",
      "Epoch 99/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 2.2169 - accuracy: 0.6026\n",
      "Epoch 100/500\n",
      "453/453 [==============================] - 0s 212us/sample - loss: 2.1903 - accuracy: 0.6203\n",
      "Epoch 101/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 2.1739 - accuracy: 0.6203\n",
      "Epoch 102/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 2.1352 - accuracy: 0.6247\n",
      "Epoch 103/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 2.1040 - accuracy: 0.6358\n",
      "Epoch 104/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 2.0867 - accuracy: 0.6358\n",
      "Epoch 105/500\n",
      "453/453 [==============================] - 0s 212us/sample - loss: 2.0701 - accuracy: 0.6380\n",
      "Epoch 106/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 2.0405 - accuracy: 0.6358\n",
      "Epoch 107/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 2.0197 - accuracy: 0.6402\n",
      "Epoch 108/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.9962 - accuracy: 0.6623\n",
      "Epoch 109/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 1.9761 - accuracy: 0.6578\n",
      "Epoch 110/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 1.9603 - accuracy: 0.6755\n",
      "Epoch 111/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 1.9384 - accuracy: 0.6733\n",
      "Epoch 112/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.9182 - accuracy: 0.6909\n",
      "Epoch 113/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 1.8975 - accuracy: 0.6909\n",
      "Epoch 114/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.8747 - accuracy: 0.7042\n",
      "Epoch 115/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 1.8722 - accuracy: 0.6821\n",
      "Epoch 116/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.8453 - accuracy: 0.6865\n",
      "Epoch 117/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.8209 - accuracy: 0.7307\n",
      "Epoch 118/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 1.7973 - accuracy: 0.7108\n",
      "Epoch 119/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.7863 - accuracy: 0.7020\n",
      "Epoch 120/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 1.7630 - accuracy: 0.7152\n",
      "Epoch 121/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 1.7602 - accuracy: 0.6976\n",
      "Epoch 122/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 1.7446 - accuracy: 0.7152\n",
      "Epoch 123/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.7231 - accuracy: 0.7064\n",
      "Epoch 124/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 1.6937 - accuracy: 0.7285\n",
      "Epoch 125/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.6749 - accuracy: 0.7263\n",
      "Epoch 126/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.6538 - accuracy: 0.7439\n",
      "Epoch 127/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 1.6323 - accuracy: 0.7417\n",
      "Epoch 128/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.6146 - accuracy: 0.7417\n",
      "Epoch 129/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.6116 - accuracy: 0.7395\n",
      "Epoch 130/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 1.6152 - accuracy: 0.7395\n",
      "Epoch 131/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 1.5831 - accuracy: 0.7506\n",
      "Epoch 132/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 1.5784 - accuracy: 0.7417\n",
      "Epoch 133/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 1.5582 - accuracy: 0.7528\n",
      "Epoch 134/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 1.5524 - accuracy: 0.7461\n",
      "Epoch 135/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.5237 - accuracy: 0.7417\n",
      "Epoch 136/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 1.5139 - accuracy: 0.7528\n",
      "Epoch 137/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.4992 - accuracy: 0.7417\n",
      "Epoch 138/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 1.4833 - accuracy: 0.7506\n",
      "Epoch 139/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.4799 - accuracy: 0.7483\n",
      "Epoch 140/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.4568 - accuracy: 0.7704\n",
      "Epoch 141/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.4379 - accuracy: 0.7704\n",
      "Epoch 142/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 1.4163 - accuracy: 0.7594\n",
      "Epoch 143/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.4227 - accuracy: 0.7550\n",
      "Epoch 144/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.3940 - accuracy: 0.7704\n",
      "Epoch 145/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.3789 - accuracy: 0.7748\n",
      "Epoch 146/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.3600 - accuracy: 0.7770\n",
      "Epoch 147/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 1.3407 - accuracy: 0.7815\n",
      "Epoch 148/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.3175 - accuracy: 0.7903\n",
      "Epoch 149/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.3006 - accuracy: 0.7991\n",
      "Epoch 150/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 1.2835 - accuracy: 0.8035\n",
      "Epoch 151/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.2735 - accuracy: 0.8013\n",
      "Epoch 152/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.2665 - accuracy: 0.7947\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 190us/sample - loss: 1.2493 - accuracy: 0.8013\n",
      "Epoch 154/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.2368 - accuracy: 0.7991\n",
      "Epoch 155/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 1.2189 - accuracy: 0.8035\n",
      "Epoch 156/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 1.2087 - accuracy: 0.8168\n",
      "Epoch 157/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 1.1913 - accuracy: 0.8190\n",
      "Epoch 158/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.1811 - accuracy: 0.8212\n",
      "Epoch 159/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.1662 - accuracy: 0.8300\n",
      "Epoch 160/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 1.1535 - accuracy: 0.8256\n",
      "Epoch 161/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.1466 - accuracy: 0.8234\n",
      "Epoch 162/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 1.1361 - accuracy: 0.8256\n",
      "Epoch 163/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.1202 - accuracy: 0.8212\n",
      "Epoch 164/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 1.1102 - accuracy: 0.8278\n",
      "Epoch 165/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 1.0960 - accuracy: 0.8322\n",
      "Epoch 166/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 1.0907 - accuracy: 0.8477\n",
      "Epoch 167/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 1.0826 - accuracy: 0.8366\n",
      "Epoch 168/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.0784 - accuracy: 0.8344\n",
      "Epoch 169/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.0623 - accuracy: 0.8322\n",
      "Epoch 170/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 1.0509 - accuracy: 0.8433\n",
      "Epoch 171/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 1.0401 - accuracy: 0.8521\n",
      "Epoch 172/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 1.0450 - accuracy: 0.8411\n",
      "Epoch 173/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 1.0306 - accuracy: 0.8455\n",
      "Epoch 174/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 1.0211 - accuracy: 0.8521\n",
      "Epoch 175/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 1.0029 - accuracy: 0.8543\n",
      "Epoch 176/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.9875 - accuracy: 0.8609\n",
      "Epoch 177/500\n",
      "453/453 [==============================] - 0s 212us/sample - loss: 0.9830 - accuracy: 0.8609\n",
      "Epoch 178/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.9727 - accuracy: 0.8631\n",
      "Epoch 179/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.9627 - accuracy: 0.8698\n",
      "Epoch 180/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.9502 - accuracy: 0.8675\n",
      "Epoch 181/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.9425 - accuracy: 0.8565\n",
      "Epoch 182/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.9279 - accuracy: 0.8742\n",
      "Epoch 183/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.9150 - accuracy: 0.8742\n",
      "Epoch 184/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.9062 - accuracy: 0.8698\n",
      "Epoch 185/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.8949 - accuracy: 0.8742\n",
      "Epoch 186/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.8859 - accuracy: 0.8786\n",
      "Epoch 187/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.8775 - accuracy: 0.8830\n",
      "Epoch 188/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.8663 - accuracy: 0.8786\n",
      "Epoch 189/500\n",
      "453/453 [==============================] - 0s 216us/sample - loss: 0.8582 - accuracy: 0.8764\n",
      "Epoch 190/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.8538 - accuracy: 0.8764\n",
      "Epoch 191/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.8474 - accuracy: 0.8808\n",
      "Epoch 192/500\n",
      "453/453 [==============================] - 0s 212us/sample - loss: 0.8395 - accuracy: 0.8830\n",
      "Epoch 193/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.8323 - accuracy: 0.8896\n",
      "Epoch 194/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.8270 - accuracy: 0.8830\n",
      "Epoch 195/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.8189 - accuracy: 0.8874\n",
      "Epoch 196/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.8265 - accuracy: 0.8874\n",
      "Epoch 197/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.8241 - accuracy: 0.8874\n",
      "Epoch 198/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.8353 - accuracy: 0.8786\n",
      "Epoch 199/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.8255 - accuracy: 0.8808\n",
      "Epoch 200/500\n",
      "453/453 [==============================] - 0s 214us/sample - loss: 0.7998 - accuracy: 0.9029\n",
      "Epoch 201/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.7885 - accuracy: 0.8985\n",
      "Epoch 202/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.7796 - accuracy: 0.9007\n",
      "Epoch 203/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.7716 - accuracy: 0.8985\n",
      "Epoch 204/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.7669 - accuracy: 0.9029\n",
      "Epoch 205/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.7629 - accuracy: 0.8985\n",
      "Epoch 206/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.7567 - accuracy: 0.8985\n",
      "Epoch 207/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.7611 - accuracy: 0.8896\n",
      "Epoch 208/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.7476 - accuracy: 0.8962\n",
      "Epoch 209/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.7308 - accuracy: 0.9029\n",
      "Epoch 210/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.7152 - accuracy: 0.9051\n",
      "Epoch 211/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.7086 - accuracy: 0.8985\n",
      "Epoch 212/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.6999 - accuracy: 0.9051\n",
      "Epoch 213/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.6946 - accuracy: 0.9029\n",
      "Epoch 214/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.6795 - accuracy: 0.9073\n",
      "Epoch 215/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.6731 - accuracy: 0.9095\n",
      "Epoch 216/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.6652 - accuracy: 0.9073\n",
      "Epoch 217/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.6581 - accuracy: 0.9073\n",
      "Epoch 218/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.6517 - accuracy: 0.9117\n",
      "Epoch 219/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.6444 - accuracy: 0.9117\n",
      "Epoch 220/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.6368 - accuracy: 0.9161\n",
      "Epoch 221/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.6322 - accuracy: 0.9139\n",
      "Epoch 222/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.6298 - accuracy: 0.9073\n",
      "Epoch 223/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.6218 - accuracy: 0.9095\n",
      "Epoch 224/500\n",
      "453/453 [==============================] - 0s 214us/sample - loss: 0.6147 - accuracy: 0.9139\n",
      "Epoch 225/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.6120 - accuracy: 0.9161\n",
      "Epoch 226/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.6067 - accuracy: 0.9161\n",
      "Epoch 227/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.6047 - accuracy: 0.9073\n",
      "Epoch 228/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.5948 - accuracy: 0.9051\n",
      "Epoch 229/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 192us/sample - loss: 0.5882 - accuracy: 0.9139\n",
      "Epoch 230/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.5819 - accuracy: 0.9139\n",
      "Epoch 231/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.5763 - accuracy: 0.9161\n",
      "Epoch 232/500\n",
      "453/453 [==============================] - 0s 207us/sample - loss: 0.5724 - accuracy: 0.9161\n",
      "Epoch 233/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.5667 - accuracy: 0.9205\n",
      "Epoch 234/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.5633 - accuracy: 0.9183\n",
      "Epoch 235/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.5707 - accuracy: 0.9205\n",
      "Epoch 236/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.5729 - accuracy: 0.9117\n",
      "Epoch 237/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.5750 - accuracy: 0.9117\n",
      "Epoch 238/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.5700 - accuracy: 0.9139\n",
      "Epoch 239/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.5945 - accuracy: 0.9095\n",
      "Epoch 240/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.6230 - accuracy: 0.8985\n",
      "Epoch 241/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.6630 - accuracy: 0.8808\n",
      "Epoch 242/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.6569 - accuracy: 0.8896\n",
      "Epoch 243/500\n",
      "453/453 [==============================] - 0s 212us/sample - loss: 0.6235 - accuracy: 0.8940\n",
      "Epoch 244/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.6025 - accuracy: 0.8962\n",
      "Epoch 245/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.6114 - accuracy: 0.8852\n",
      "Epoch 246/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.5579 - accuracy: 0.9051\n",
      "Epoch 247/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.5442 - accuracy: 0.9161\n",
      "Epoch 248/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.5309 - accuracy: 0.9205\n",
      "Epoch 249/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.5335 - accuracy: 0.9249\n",
      "Epoch 250/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.5186 - accuracy: 0.9272\n",
      "Epoch 251/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.5139 - accuracy: 0.9272\n",
      "Epoch 252/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.5195 - accuracy: 0.9249\n",
      "Epoch 253/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.5260 - accuracy: 0.9227\n",
      "Epoch 254/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.5252 - accuracy: 0.9139\n",
      "Epoch 255/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.5050 - accuracy: 0.9205\n",
      "Epoch 256/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.4904 - accuracy: 0.9272\n",
      "Epoch 257/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.4784 - accuracy: 0.9249\n",
      "Epoch 258/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.4708 - accuracy: 0.9294\n",
      "Epoch 259/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.4865 - accuracy: 0.9249\n",
      "Epoch 260/500\n",
      "453/453 [==============================] - 0s 207us/sample - loss: 0.4639 - accuracy: 0.9338\n",
      "Epoch 261/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.4578 - accuracy: 0.9338\n",
      "Epoch 262/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.4524 - accuracy: 0.9338\n",
      "Epoch 263/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.4478 - accuracy: 0.9360\n",
      "Epoch 264/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.4460 - accuracy: 0.9360\n",
      "Epoch 265/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.4411 - accuracy: 0.9360\n",
      "Epoch 266/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.4375 - accuracy: 0.9316\n",
      "Epoch 267/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.4358 - accuracy: 0.9338\n",
      "Epoch 268/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.4302 - accuracy: 0.9360\n",
      "Epoch 269/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.4272 - accuracy: 0.9382\n",
      "Epoch 270/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.4242 - accuracy: 0.9404\n",
      "Epoch 271/500\n",
      "453/453 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.94 - 0s 194us/sample - loss: 0.4205 - accuracy: 0.9426\n",
      "Epoch 272/500\n",
      "453/453 [==============================] - 0s 212us/sample - loss: 0.4171 - accuracy: 0.9404\n",
      "Epoch 273/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.4156 - accuracy: 0.9404\n",
      "Epoch 274/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.4209 - accuracy: 0.9382\n",
      "Epoch 275/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.4275 - accuracy: 0.9382\n",
      "Epoch 276/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.4188 - accuracy: 0.9404\n",
      "Epoch 277/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.4155 - accuracy: 0.9382\n",
      "Epoch 278/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.4090 - accuracy: 0.9382\n",
      "Epoch 279/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.4103 - accuracy: 0.9316\n",
      "Epoch 280/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.4197 - accuracy: 0.9360\n",
      "Epoch 281/500\n",
      "453/453 [==============================] - 0s 219us/sample - loss: 0.4152 - accuracy: 0.9316\n",
      "Epoch 282/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.4083 - accuracy: 0.9294\n",
      "Epoch 283/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.3973 - accuracy: 0.9338\n",
      "Epoch 284/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.3878 - accuracy: 0.9338 - loss: 0.3842 - accuracy: 0.92\n",
      "Epoch 285/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.3849 - accuracy: 0.9382\n",
      "Epoch 286/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.3807 - accuracy: 0.9382\n",
      "Epoch 287/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.3792 - accuracy: 0.9382\n",
      "Epoch 288/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3753 - accuracy: 0.9426\n",
      "Epoch 289/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3715 - accuracy: 0.9404\n",
      "Epoch 290/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.3692 - accuracy: 0.9448\n",
      "Epoch 291/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.3668 - accuracy: 0.9426\n",
      "Epoch 292/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.3642 - accuracy: 0.9426\n",
      "Epoch 293/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.3620 - accuracy: 0.9426\n",
      "Epoch 294/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3588 - accuracy: 0.9448\n",
      "Epoch 295/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.3560 - accuracy: 0.9448\n",
      "Epoch 296/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.3531 - accuracy: 0.9448\n",
      "Epoch 297/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.3518 - accuracy: 0.9448\n",
      "Epoch 298/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.3493 - accuracy: 0.9426\n",
      "Epoch 299/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.3464 - accuracy: 0.9426\n",
      "Epoch 300/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.3443 - accuracy: 0.9448\n",
      "Epoch 301/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3424 - accuracy: 0.9448\n",
      "Epoch 302/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.3392 - accuracy: 0.9448\n",
      "Epoch 303/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.3379 - accuracy: 0.9448\n",
      "Epoch 304/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.3349 - accuracy: 0.9426\n",
      "Epoch 305/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 201us/sample - loss: 0.3334 - accuracy: 0.9448\n",
      "Epoch 306/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.3302 - accuracy: 0.9404\n",
      "Epoch 307/500\n",
      "453/453 [==============================] - 0s 208us/sample - loss: 0.3280 - accuracy: 0.9448\n",
      "Epoch 308/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.3264 - accuracy: 0.9448\n",
      "Epoch 309/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3254 - accuracy: 0.9448\n",
      "Epoch 310/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.3224 - accuracy: 0.9470\n",
      "Epoch 311/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.3200 - accuracy: 0.9426\n",
      "Epoch 312/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3179 - accuracy: 0.9448\n",
      "Epoch 313/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3175 - accuracy: 0.9470\n",
      "Epoch 314/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.3142 - accuracy: 0.9470\n",
      "Epoch 315/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.3124 - accuracy: 0.9470\n",
      "Epoch 316/500\n",
      "453/453 [==============================] - 0s 205us/sample - loss: 0.3098 - accuracy: 0.9470\n",
      "Epoch 317/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.3076 - accuracy: 0.9448\n",
      "Epoch 318/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.3067 - accuracy: 0.9470\n",
      "Epoch 319/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.3046 - accuracy: 0.9448\n",
      "Epoch 320/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.3025 - accuracy: 0.9470\n",
      "Epoch 321/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3010 - accuracy: 0.9470\n",
      "Epoch 322/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2996 - accuracy: 0.9448\n",
      "Epoch 323/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2974 - accuracy: 0.9448\n",
      "Epoch 324/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2955 - accuracy: 0.9426\n",
      "Epoch 325/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2937 - accuracy: 0.9448\n",
      "Epoch 326/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2919 - accuracy: 0.9448\n",
      "Epoch 327/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.2940 - accuracy: 0.9404\n",
      "Epoch 328/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.3106 - accuracy: 0.9316\n",
      "Epoch 329/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.3127 - accuracy: 0.9316\n",
      "Epoch 330/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.3228 - accuracy: 0.9360\n",
      "Epoch 331/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3393 - accuracy: 0.9316\n",
      "Epoch 332/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.3030 - accuracy: 0.9382\n",
      "Epoch 333/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3154 - accuracy: 0.9404\n",
      "Epoch 334/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.3401 - accuracy: 0.9316\n",
      "Epoch 335/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.3648 - accuracy: 0.9272\n",
      "Epoch 336/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.3748 - accuracy: 0.9183\n",
      "Epoch 337/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.3585 - accuracy: 0.9272\n",
      "Epoch 338/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.3311 - accuracy: 0.9338\n",
      "Epoch 339/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.3162 - accuracy: 0.9382\n",
      "Epoch 340/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2982 - accuracy: 0.9360\n",
      "Epoch 341/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2850 - accuracy: 0.9426\n",
      "Epoch 342/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2761 - accuracy: 0.9448\n",
      "Epoch 343/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2745 - accuracy: 0.9404\n",
      "Epoch 344/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2712 - accuracy: 0.9404\n",
      "Epoch 345/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2681 - accuracy: 0.9448\n",
      "Epoch 346/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2666 - accuracy: 0.9426\n",
      "Epoch 347/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2663 - accuracy: 0.9448\n",
      "Epoch 348/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2634 - accuracy: 0.9470\n",
      "Epoch 349/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2605 - accuracy: 0.9470\n",
      "Epoch 350/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2594 - accuracy: 0.9382\n",
      "Epoch 351/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.2582 - accuracy: 0.9426\n",
      "Epoch 352/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2573 - accuracy: 0.9448\n",
      "Epoch 353/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.2572 - accuracy: 0.9426\n",
      "Epoch 354/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2544 - accuracy: 0.9426\n",
      "Epoch 355/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2518 - accuracy: 0.9404\n",
      "Epoch 356/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2503 - accuracy: 0.9448\n",
      "Epoch 357/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2488 - accuracy: 0.9426\n",
      "Epoch 358/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.2476 - accuracy: 0.9426\n",
      "Epoch 359/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2459 - accuracy: 0.9404\n",
      "Epoch 360/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2439 - accuracy: 0.9382\n",
      "Epoch 361/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2430 - accuracy: 0.9426\n",
      "Epoch 362/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.2410 - accuracy: 0.9426\n",
      "Epoch 363/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2402 - accuracy: 0.9426\n",
      "Epoch 364/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2388 - accuracy: 0.9426\n",
      "Epoch 365/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2374 - accuracy: 0.9448\n",
      "Epoch 366/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.2370 - accuracy: 0.9470\n",
      "Epoch 367/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.2354 - accuracy: 0.9448\n",
      "Epoch 368/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.2349 - accuracy: 0.9470\n",
      "Epoch 369/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.2318 - accuracy: 0.9470\n",
      "Epoch 370/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2306 - accuracy: 0.9514\n",
      "Epoch 371/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2298 - accuracy: 0.9448\n",
      "Epoch 372/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.2283 - accuracy: 0.9470\n",
      "Epoch 373/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2269 - accuracy: 0.9448\n",
      "Epoch 374/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2255 - accuracy: 0.9470\n",
      "Epoch 375/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2245 - accuracy: 0.9536\n",
      "Epoch 376/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2239 - accuracy: 0.9492\n",
      "Epoch 377/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2218 - accuracy: 0.9514\n",
      "Epoch 378/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.2214 - accuracy: 0.9514\n",
      "Epoch 379/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2196 - accuracy: 0.9514\n",
      "Epoch 380/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.2186 - accuracy: 0.9514\n",
      "Epoch 381/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2189 - accuracy: 0.9514\n",
      "Epoch 382/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2202 - accuracy: 0.9514\n",
      "Epoch 383/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2250 - accuracy: 0.9470\n",
      "Epoch 384/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2245 - accuracy: 0.9470\n",
      "Epoch 385/500\n",
      "453/453 [==============================] - 0s 193us/sample - loss: 0.2195 - accuracy: 0.9470\n",
      "Epoch 386/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.2187 - accuracy: 0.9514\n",
      "Epoch 387/500\n",
      "453/453 [==============================] - 0s 198us/sample - loss: 0.2164 - accuracy: 0.9470\n",
      "Epoch 388/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.2163 - accuracy: 0.9492\n",
      "Epoch 389/500\n",
      "453/453 [==============================] - 0s 197us/sample - loss: 0.2114 - accuracy: 0.9536\n",
      "Epoch 390/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.2102 - accuracy: 0.9492\n",
      "Epoch 391/500\n",
      "453/453 [==============================] - 0s 187us/sample - loss: 0.2083 - accuracy: 0.9492\n",
      "Epoch 392/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2076 - accuracy: 0.9448\n",
      "Epoch 393/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.2064 - accuracy: 0.9492\n",
      "Epoch 394/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2057 - accuracy: 0.9448\n",
      "Epoch 395/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.2043 - accuracy: 0.9492\n",
      "Epoch 396/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.2035 - accuracy: 0.9492\n",
      "Epoch 397/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.2029 - accuracy: 0.9492\n",
      "Epoch 398/500\n",
      "453/453 [==============================] - 0s 195us/sample - loss: 0.2013 - accuracy: 0.9470\n",
      "Epoch 399/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.2006 - accuracy: 0.9492\n",
      "Epoch 400/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1999 - accuracy: 0.9492\n",
      "Epoch 401/500\n",
      "453/453 [==============================] - 0s 193us/sample - loss: 0.1989 - accuracy: 0.9470\n",
      "Epoch 402/500\n",
      "453/453 [==============================] - 0s 195us/sample - loss: 0.1977 - accuracy: 0.9492\n",
      "Epoch 403/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1964 - accuracy: 0.9492\n",
      "Epoch 404/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1962 - accuracy: 0.9492\n",
      "Epoch 405/500\n",
      "453/453 [==============================] - 0s 197us/sample - loss: 0.1957 - accuracy: 0.9470\n",
      "Epoch 406/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1961 - accuracy: 0.9470\n",
      "Epoch 407/500\n",
      "453/453 [==============================] - 0s 195us/sample - loss: 0.1940 - accuracy: 0.9470\n",
      "Epoch 408/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1941 - accuracy: 0.9536\n",
      "Epoch 409/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.1923 - accuracy: 0.9536\n",
      "Epoch 410/500\n",
      "453/453 [==============================] - 0s 187us/sample - loss: 0.1912 - accuracy: 0.9492\n",
      "Epoch 411/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1923 - accuracy: 0.9492\n",
      "Epoch 412/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.1902 - accuracy: 0.9492\n",
      "Epoch 413/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.1896 - accuracy: 0.9514\n",
      "Epoch 414/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1884 - accuracy: 0.9470\n",
      "Epoch 415/500\n",
      "453/453 [==============================] - 0s 201us/sample - loss: 0.1871 - accuracy: 0.9514\n",
      "Epoch 416/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1878 - accuracy: 0.9492\n",
      "Epoch 417/500\n",
      "453/453 [==============================] - 0s 203us/sample - loss: 0.2285 - accuracy: 0.9404\n",
      "Epoch 418/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.2497 - accuracy: 0.9294\n",
      "Epoch 419/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.2455 - accuracy: 0.9360\n",
      "Epoch 420/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.2648 - accuracy: 0.9338\n",
      "Epoch 421/500\n",
      "453/453 [==============================] - 0s 195us/sample - loss: 0.2278 - accuracy: 0.9470\n",
      "Epoch 422/500\n",
      "453/453 [==============================] - 0s 198us/sample - loss: 0.2337 - accuracy: 0.9448\n",
      "Epoch 423/500\n",
      "453/453 [==============================] - 0s 202us/sample - loss: 0.2201 - accuracy: 0.9492\n",
      "Epoch 424/500\n",
      "453/453 [==============================] - 0s 197us/sample - loss: 0.2120 - accuracy: 0.9448\n",
      "Epoch 425/500\n",
      "453/453 [==============================] - 0s 193us/sample - loss: 0.2073 - accuracy: 0.9426\n",
      "Epoch 426/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1975 - accuracy: 0.9514\n",
      "Epoch 427/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1976 - accuracy: 0.9514\n",
      "Epoch 428/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1884 - accuracy: 0.9470\n",
      "Epoch 429/500\n",
      "453/453 [==============================] - 0s 200us/sample - loss: 0.1865 - accuracy: 0.9492\n",
      "Epoch 430/500\n",
      "453/453 [==============================] - 0s 193us/sample - loss: 0.1846 - accuracy: 0.9536\n",
      "Epoch 431/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1837 - accuracy: 0.9514\n",
      "Epoch 432/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1824 - accuracy: 0.9514\n",
      "Epoch 433/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1816 - accuracy: 0.9492\n",
      "Epoch 434/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1815 - accuracy: 0.9514\n",
      "Epoch 435/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.1802 - accuracy: 0.9514\n",
      "Epoch 436/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1791 - accuracy: 0.9514\n",
      "Epoch 437/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1789 - accuracy: 0.9514\n",
      "Epoch 438/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1794 - accuracy: 0.9448\n",
      "Epoch 439/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1773 - accuracy: 0.9492\n",
      "Epoch 440/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1768 - accuracy: 0.9448\n",
      "Epoch 441/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1751 - accuracy: 0.9470\n",
      "Epoch 442/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1738 - accuracy: 0.9404\n",
      "Epoch 443/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.1743 - accuracy: 0.9448\n",
      "Epoch 444/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.1721 - accuracy: 0.9492\n",
      "Epoch 445/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1717 - accuracy: 0.9448\n",
      "Epoch 446/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1708 - accuracy: 0.9470\n",
      "Epoch 447/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1698 - accuracy: 0.9470\n",
      "Epoch 448/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1688 - accuracy: 0.9492\n",
      "Epoch 449/500\n",
      "453/453 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.94 - 0s 194us/sample - loss: 0.1687 - accuracy: 0.9426\n",
      "Epoch 450/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1679 - accuracy: 0.9448\n",
      "Epoch 451/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1678 - accuracy: 0.9448\n",
      "Epoch 452/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1664 - accuracy: 0.9426\n",
      "Epoch 453/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1663 - accuracy: 0.9426\n",
      "Epoch 454/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1649 - accuracy: 0.9536\n",
      "Epoch 455/500\n",
      "453/453 [==============================] - 0s 181us/sample - loss: 0.1652 - accuracy: 0.9514\n",
      "Epoch 456/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1647 - accuracy: 0.9492\n",
      "Epoch 457/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1653 - accuracy: 0.9514\n",
      "Epoch 458/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.1632 - accuracy: 0.9514\n",
      "Epoch 459/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.1632 - accuracy: 0.9514\n",
      "Epoch 460/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1630 - accuracy: 0.9470\n",
      "Epoch 461/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.1617 - accuracy: 0.9470\n",
      "Epoch 462/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1613 - accuracy: 0.9514\n",
      "Epoch 463/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1612 - accuracy: 0.9448\n",
      "Epoch 464/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.1603 - accuracy: 0.9404\n",
      "Epoch 465/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1604 - accuracy: 0.9492\n",
      "Epoch 466/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1593 - accuracy: 0.9514\n",
      "Epoch 467/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1589 - accuracy: 0.9536\n",
      "Epoch 468/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.1587 - accuracy: 0.9470\n",
      "Epoch 469/500\n",
      "453/453 [==============================] - 0s 210us/sample - loss: 0.1570 - accuracy: 0.9514\n",
      "Epoch 470/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1573 - accuracy: 0.9536\n",
      "Epoch 471/500\n",
      "453/453 [==============================] - 0s 199us/sample - loss: 0.1568 - accuracy: 0.9492\n",
      "Epoch 472/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1560 - accuracy: 0.9492\n",
      "Epoch 473/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1558 - accuracy: 0.9448\n",
      "Epoch 474/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1555 - accuracy: 0.9470\n",
      "Epoch 475/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1544 - accuracy: 0.9514\n",
      "Epoch 476/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1545 - accuracy: 0.9514\n",
      "Epoch 477/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1528 - accuracy: 0.9536\n",
      "Epoch 478/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1550 - accuracy: 0.9470\n",
      "Epoch 479/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1528 - accuracy: 0.9470\n",
      "Epoch 480/500\n",
      "453/453 [==============================] - 0s 194us/sample - loss: 0.1519 - accuracy: 0.9426\n",
      "Epoch 481/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1512 - accuracy: 0.9514\n",
      "Epoch 482/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1505 - accuracy: 0.9492\n",
      "Epoch 483/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1524 - accuracy: 0.9448\n",
      "Epoch 484/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1526 - accuracy: 0.9470\n",
      "Epoch 485/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1551 - accuracy: 0.9514\n",
      "Epoch 486/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.1571 - accuracy: 0.9492\n",
      "Epoch 487/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.1518 - accuracy: 0.9470\n",
      "Epoch 488/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1487 - accuracy: 0.9514\n",
      "Epoch 489/500\n",
      "453/453 [==============================] - 0s 191us/sample - loss: 0.1487 - accuracy: 0.9514\n",
      "Epoch 490/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1476 - accuracy: 0.9536\n",
      "Epoch 491/500\n",
      "453/453 [==============================] - 0s 190us/sample - loss: 0.1466 - accuracy: 0.9514\n",
      "Epoch 492/500\n",
      "453/453 [==============================] - 0s 188us/sample - loss: 0.1465 - accuracy: 0.9514\n",
      "Epoch 493/500\n",
      "453/453 [==============================] - 0s 191us/sample - loss: 0.1461 - accuracy: 0.9492\n",
      "Epoch 494/500\n",
      "453/453 [==============================] - 0s 187us/sample - loss: 0.1459 - accuracy: 0.9426\n",
      "Epoch 495/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1457 - accuracy: 0.9470\n",
      "Epoch 496/500\n",
      "453/453 [==============================] - 0s 183us/sample - loss: 0.1453 - accuracy: 0.9426\n",
      "Epoch 497/500\n",
      "453/453 [==============================] - 0s 185us/sample - loss: 0.1445 - accuracy: 0.9448\n",
      "Epoch 498/500\n",
      "453/453 [==============================] - 0s 196us/sample - loss: 0.1448 - accuracy: 0.9448\n",
      "Epoch 499/500\n",
      "453/453 [==============================] - 0s 193us/sample - loss: 0.1439 - accuracy: 0.9514\n",
      "Epoch 500/500\n",
      "453/453 [==============================] - 0s 192us/sample - loss: 0.1432 - accuracy: 0.9448\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words,64,input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "hist = model.fit(xs,ys,epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e84061d748>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laurence went to dublin round athy one jeremy lanigan eyes glisten glisten glisten glisten glisten glisten glisten put nice gray me them the a jig jig jig jig able polkas her a your glisten glisten glisten glisten glisten died me them of gray and the wall man all the girls they got a call call ask glisten glisten died me them me a polkas polkas polkas polkas red a call water leg eyes gray me creature up fainted gray gray relations relations relations a cask man and father cask cask glisten glisten glisten glisten died me them of the wall jig cask polkas red\n"
     ]
    }
   ],
   "source": [
    "seed_txt = \"Laurence went to dublin\"\n",
    "next_words = 100\n",
    "for i in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_txt])[0]\n",
    "    token_list = pad_sequences([token_list],maxlen = max_sequence_len-1,padding='pre')\n",
    "    predicted = model.predict_classes(token_list)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted :\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_txt = seed_txt + \" \" +output_word\n",
    "print(seed_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
